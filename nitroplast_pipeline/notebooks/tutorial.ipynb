{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nitroplast Import Code Discovery Tutorial\n",
    "\n",
    "This notebook demonstrates the complete pipeline for discovering hidden protein targeting signals using ESM-2 embeddings and supervised contrastive learning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Goal**: Identify the \"hidden\" targeting signals in 148 nitroplast-localized proteins that lack the known uTP motif.\n",
    "\n",
    "**Approach**:\n",
    "1. Train ESM-2 + LoRA to distinguish nitroplast from cytosolic proteins\n",
    "2. Use supervised contrastive learning to group all positives (uTP+ and uTP-) together\n",
    "3. Analyze attention weights to discover what the model \"sees\" in the hidden proteins\n",
    "4. Predict novel nitroplast-localized proteins in the full proteome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's first understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import load_fasta, validate_sequences\n",
    "\n",
    "# Load data\n",
    "pos_sequences, pos_ids = load_fasta('../data/raw/nitroplast_proteins.fasta')\n",
    "neg_sequences, neg_ids = load_fasta('../data/raw/host_cytosolic.fasta')\n",
    "\n",
    "print(f\"Positive (nitroplast) proteins: {len(pos_sequences)}\")\n",
    "print(f\"Negative (cytosolic) proteins: {len(neg_sequences)}\")\n",
    "\n",
    "# Sequence length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist([len(s) for s in pos_sequences], bins=30, alpha=0.7, label='Nitroplast', color='red')\n",
    "axes[0].hist([len(s) for s in neg_sequences], bins=30, alpha=0.7, label='Cytosolic', color='blue')\n",
    "axes[0].set_xlabel('Sequence Length')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Sequence Length Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Average length comparison\n",
    "lengths = {\n",
    "    'Nitroplast': [len(s) for s in pos_sequences],\n",
    "    'Cytosolic': [len(s) for s in neg_sequences]\n",
    "}\n",
    "axes[1].boxplot(lengths.values(), labels=lengths.keys())\n",
    "axes[1].set_ylabel('Sequence Length')\n",
    "axes[1].set_title('Length Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage length (nitroplast): {np.mean([len(s) for s in pos_sequences]):.1f} aa\")\n",
    "print(f\"Average length (cytosolic): {np.mean([len(s) for s in neg_sequences]):.1f} aa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We'll use sequence similarity-based splitting to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import prepare_datasets\n",
    "\n",
    "# Load config\n",
    "with open('../configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
    "    positive_fasta='../data/raw/nitroplast_proteins.fasta',\n",
    "    negative_fasta='../data/raw/host_cytosolic.fasta',\n",
    "    output_dir='../data/processed/',\n",
    "    config=config,\n",
    "    force_recompute=False\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} sequences\")\n",
    "print(f\"Val: {len(val_dataset)} sequences\")\n",
    "print(f\"Test: {len(test_dataset)} sequences\")\n",
    "\n",
    "# Check label distribution\n",
    "for name, dataset in [('Train', train_dataset), ('Val', val_dataset), ('Test', test_dataset)]:\n",
    "    n_pos = sum(dataset.labels)\n",
    "    n_neg = len(dataset) - n_pos\n",
    "    print(f\"\\n{name} set:\")\n",
    "    print(f\"  Positive: {n_pos} ({n_pos/len(dataset)*100:.1f}%)\")\n",
    "    print(f\"  Negative: {n_neg} ({n_neg/len(dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "Our model consists of:\n",
    "1. **ESM-2 Encoder** (650M parameters) with LoRA adaptation\n",
    "2. **Projection Head** that maps to 128-dimensional contrastive space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.esm_encoder import ESMEncoder\n",
    "from models.projector import ProjectionHead, NitroplastContrastiveModel\n",
    "\n",
    "# Build encoder\n",
    "encoder = ESMEncoder(\n",
    "    model_name=config['model']['esm_model_name'],\n",
    "    use_lora=config['model']['use_lora'],\n",
    "    lora_config={\n",
    "        'r': config['model']['lora_r'],\n",
    "        'alpha': config['model']['lora_alpha'],\n",
    "        'dropout': config['model']['lora_dropout'],\n",
    "        'target_modules': config['model']['lora_target_modules']\n",
    "    },\n",
    "    freeze_layers=config['model']['freeze_esm_layers'],\n",
    "    pooling_method=config['model']['pooling_method'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Build projection head\n",
    "projector = ProjectionHead(\n",
    "    input_dim=config['model']['projector']['input_dim'],\n",
    "    hidden_dims=config['model']['projector']['hidden_dims'],\n",
    "    output_dim=config['model']['projector']['output_dim'],\n",
    "    dropout=config['model']['projector']['dropout'],\n",
    "    use_batch_norm=config['model']['projector']['use_batch_norm']\n",
    ")\n",
    "\n",
    "# Combine into full model\n",
    "model = NitroplastContrastiveModel(encoder, projector).to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Forward Pass\n",
    "\n",
    "Verify the model works with sample sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample sequences\n",
    "test_sequences = train_dataset.sequences[:4]\n",
    "test_labels = torch.tensor(train_dataset.labels[:4])\n",
    "\n",
    "print(\"Sample sequences:\")\n",
    "for i, (seq, label) in enumerate(zip(test_sequences, test_labels)):\n",
    "    print(f\"{i+1}. Length: {len(seq)}, Label: {'Nitroplast' if label == 1 else 'Cytosolic'}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_sequences)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  ESM-2 embeddings: {outputs['embeddings'].shape}\")\n",
    "print(f\"  Contrastive embeddings: {outputs['projected'].shape}\")\n",
    "print(f\"  Residue embeddings: {outputs['residue_embeddings'].shape}\")\n",
    "\n",
    "# Check L2 normalization\n",
    "norms = outputs['projected'].norm(dim=1)\n",
    "print(f\"\\nL2 norms (should be ~1.0): {norms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training (Simplified Version)\n",
    "\n",
    "For the full training, run: `python train.py --config configs/config.yaml`\n",
    "\n",
    "Here we'll demonstrate a few training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.training_utils import collate_fn, create_optimizer\n",
    "from models.projector import SupConLoss\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Setup training\n",
    "loss_fn = SupConLoss(temperature=config['training']['temperature'])\n",
    "optimizer = create_optimizer(model, learning_rate=2e-4, weight_decay=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "sequences = batch['sequences']\n",
    "labels = batch['labels'].to(device)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(sequences)\n",
    "projected = outputs['projected']\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(projected, labels)\n",
    "\n",
    "print(f\"Batch size: {len(sequences)}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\n✓ Training step completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Pre-trained Model\n",
    "\n",
    "After training completes, load the best model for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inference_utils import load_model_for_inference\n",
    "\n",
    "# Load trained model\n",
    "checkpoint_path = '../results/checkpoints/best_model.pt'\n",
    "\n",
    "if Path(checkpoint_path).exists():\n",
    "    model = load_model_for_inference(\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        config=config,\n",
    "        device=device\n",
    "    )\n",
    "    print(\"✓ Model loaded successfully\")\n",
    "else:\n",
    "    print(\"⚠ Model not found. Run training first: python train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Learned Embeddings\n",
    "\n",
    "Use t-SNE to visualize how well the model separates nitroplast from cytosolic proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import visualize_embeddings\n",
    "\n",
    "# Compute embeddings for test set\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_ids = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        sequences = batch['sequences']\n",
    "        labels = batch['labels']\n",
    "        ids = batch['ids']\n",
    "        \n",
    "        embeddings = model.get_embeddings(sequences).cpu().numpy()\n",
    "        \n",
    "        all_embeddings.append(embeddings)\n",
    "        all_labels.append(labels.numpy())\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "labels = np.concatenate(all_labels)\n",
    "\n",
    "# Visualize\n",
    "visualize_embeddings(\n",
    "    embeddings=embeddings,\n",
    "    labels=labels,\n",
    "    ids=all_ids,\n",
    "    output_path='../results/plots/test_embeddings.png',\n",
    "    method='tsne',\n",
    "    title='Test Set Protein Embeddings'\n",
    ")\n",
    "\n",
    "print(\"✓ Embedding visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention Analysis\n",
    "\n",
    "Analyze what the model is attending to in the 148 \"hidden\" proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualization import visualize_attention_map, extract_high_attention_regions\n",
    "\n",
    "# Load a hidden protein (no uTP motif)\n",
    "# For demonstration, use first test protein\n",
    "test_protein_id = test_dataset.ids[0]\n",
    "test_sequence = test_dataset.sequences[0]\n",
    "\n",
    "print(f\"Analyzing: {test_protein_id}\")\n",
    "print(f\"Length: {len(test_sequence)} aa\")\n",
    "\n",
    "# Get attention weights\n",
    "attention = model.encoder.get_attention_weights(\n",
    "    [test_sequence],\n",
    "    layer_idx=-1,\n",
    "    aggregate_heads='mean'\n",
    ")\n",
    "attention = attention[0].cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "visualize_attention_map(\n",
    "    sequence=test_sequence,\n",
    "    attention=attention,\n",
    "    protein_id=test_protein_id,\n",
    "    output_path=f'../results/attention_maps/{test_protein_id}_attention.png',\n",
    "    top_k_residues=20,\n",
    "    window_size=30\n",
    ")\n",
    "\n",
    "# Extract high-attention regions\n",
    "regions = extract_high_attention_regions(\n",
    "    sequence=test_sequence,\n",
    "    attention=attention,\n",
    "    window_size=15,\n",
    "    min_attention=0.1\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 5 high-attention regions:\")\n",
    "for i, region in enumerate(regions[:5], 1):\n",
    "    print(f\"{i}. Position {region['position']}: {region['sequence']} (score: {region['avg_attention']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Zero-Shot Prediction\n",
    "\n",
    "Predict novel nitroplast-localized proteins in the full proteome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.inference_utils import NitroplastPredictor, compute_reference_embeddings\n",
    "\n",
    "# Compute reference embeddings\n",
    "ref_embeddings, ref_labels, ref_ids = compute_reference_embeddings(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create predictor\n",
    "predictor = NitroplastPredictor(\n",
    "    model=model,\n",
    "    reference_embeddings=ref_embeddings,\n",
    "    reference_labels=ref_labels,\n",
    "    reference_ids=ref_ids,\n",
    "    distance_metric='cosine',\n",
    "    confidence_threshold=0.8,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Example prediction\n",
    "example_seq = test_dataset.sequences[10]\n",
    "example_id = test_dataset.ids[10]\n",
    "\n",
    "prediction = predictor.predict_single(example_seq, example_id)\n",
    "\n",
    "print(f\"Protein: {prediction['protein_id']}\")\n",
    "print(f\"Prediction: {'Nitroplast' if prediction['prediction'] == 1 else 'Cytosolic'}\")\n",
    "print(f\"Confidence: {prediction['confidence']:.3f}\")\n",
    "print(f\"\\nNearest neighbors:\")\n",
    "for i, nn in enumerate(prediction['nearest_neighbors'], 1):\n",
    "    label = 'Nitroplast' if nn['label'] == 1 else 'Cytosolic'\n",
    "    print(f\"  {i}. {nn['id']} ({label}), distance: {nn['distance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished:\n",
    "1. ✓ Built a contrastive learning model to distinguish nitroplast proteins\n",
    "2. ✓ Visualized learned embedding space\n",
    "3. ✓ Analyzed attention patterns to discover hidden signals\n",
    "4. ✓ Made zero-shot predictions on novel proteins\n",
    "\n",
    "### Next Steps:\n",
    "1. **Analyze all 148 hidden proteins** systematically\n",
    "   - Run: `python analyze_attention.py --checkpoint results/checkpoints/best_model.pt ...`\n",
    "2. **Look for consensus motifs** in high-attention regions\n",
    "3. **Predict full proteome**\n",
    "   - Run: `python predict.py --checkpoint ... --proteome ...`\n",
    "4. **Validate discoveries experimentally**\n",
    "   - Test predicted proteins for nitroplast localization\n",
    "   - Mutate high-attention regions to test functionality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
